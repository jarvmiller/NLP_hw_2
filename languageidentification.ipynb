{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'ã', 'æ', 'ç', 'è', 'é', 'ê', 'ì', 'í', 'î', 'ï', 'ò', 'ó', 'ô', 'ù', 'ú', 'û', 'ü', 'ÿ', 'œ', ' ', '!', '?', '¿', '¡', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] 94\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_array(x):\n",
    "    # input: array\n",
    "    # output: sigmoid applied to each value of input array\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    # input: array\n",
    "    # output: softmax of array\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def define_alphabet():\n",
    "    # creates list of alphabet to use when parsing text data\n",
    "    base_en = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    special_chars = ' !?¿¡' + string.punctuation + string.digits\n",
    "    italian = 'àèéìíòóùúã'\n",
    "    french = 'àâæçéèêêîïôœùûüÿ'\n",
    "    all_lang_chars = base_en + italian + french \n",
    "    small_chars = list(set(list(all_lang_chars)))\n",
    "    small_chars.sort() \n",
    "    big_chars = list(set(list(all_lang_chars.upper())))\n",
    "    big_chars.sort()\n",
    "    small_chars += special_chars\n",
    "    letters_string = ''\n",
    "    letters = small_chars + big_chars\n",
    "    for letter in letters:\n",
    "        letters_string += letter\n",
    "    return small_chars,big_chars,letters_string\n",
    "\n",
    "alphabet, _, _ = define_alphabet()\n",
    "print(alphabet, len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits labelBinarizer obj to alphabet so it can make one-hot encoding\n",
    "# c = unique characters between eng, fre, ital = 93\n",
    "# le = LabelBinarizer()\n",
    "# le.fit(alphabet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ' '\n",
    "with open(\"languageIdentification.data/train\", 'r') as f:\n",
    "    txt += f.read()\n",
    "# with open(\"languageIdentification.data/dev\", 'r') as f:\n",
    "#     txt += f.read()\n",
    "# with open(\"languageIdentification.data/test\", 'r',encoding = 'latin-1') as f:\n",
    "#     txt += f.read()\n",
    "alphab = list(set(list(txt.lower())))\n",
    "input_dim = (len(alphab)) * 5\n",
    "# print(len(alphab), input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelBinarizer()\n",
    "le.fit(alphab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_chars(s, num_chars=5):\n",
    "    # input: string, s\n",
    "    # output: list of 5 sequential characters of string from beginning to end\n",
    "    n = len(s)\n",
    "    return [s[i:(i+num_chars)] for i in range(n-4)]\n",
    "\n",
    "def binarize(seq_str):\n",
    "    # input: sequence of characters\n",
    "    # output: concatenated one hot encoding of each character\n",
    "        # w/ dimension (5c, 1)\n",
    "    nseq = len(seq_str)\n",
    "    if len(seq_str) == 0:\n",
    "        return np.zeros(shape = (1,input_dim))\n",
    "    for char in seq_str[0]:\n",
    "        if char not in alphab:\n",
    "            print(char, 'not in alphabet')\n",
    "    return np.array([le.transform(list(seq_str[i])) for i in range(nseq)]).reshape(nseq, input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if youve not seen a char before, it's all zeros.. this may be the runtime error\n",
    "# binarize(seq_chars('ㄱㄱabㄱ')).shape\n",
    "# arr = []\n",
    "# for char in 'ㄱㄱabㄱ':\n",
    "#     if char not in alphab:\n",
    "#         arr.append(np.append(le.transform([char])[0], 1))\n",
    "#     else:\n",
    "#         arr.append(np.append(le.transform([char])[0], 0))\n",
    "# # [len(i) for i in arr]\n",
    "# np.concatenate([np.array(i) for i in arr]).reshape(1,input_dim).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{y} L = y - \\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_y(y_pred, y_test):\n",
    "    return y_pred - y_test #(3,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{b^{2}} L = \\nabla_{y^{'}}L = \\sum_{i} \\frac{\\delta{L}}{\\delta{y_{i}}} y_{i}(\\delta{ij} - y_{j}) = \\sum_{i} (y_{i} - \\hat{y_{i}}) y_{i}(\\delta_{ij} - y_{j})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_b2(y_pred, y_test):\n",
    "    vec = []\n",
    "    # for each yj\n",
    "    for j, val in enumerate(list(y_pred)):\n",
    "        counter = 0\n",
    "        # go over all values in yi\n",
    "        for i, val2 in enumerate(list(y_pred)):\n",
    "            if i == j:\n",
    "                counter += (y_pred[i] - y_test[i])*(y_pred[i])*(1-y_pred[j])\n",
    "            else:\n",
    "                counter += (y_pred[i] - y_test[i])*(y_pred[i])*(-y_pred[j])\n",
    "        vec.append(counter)\n",
    "    return np.array(vec)\n",
    "\n",
    "     \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{w^{2}} L = \\nabla_{y^{'}}L h^{T} = \\nabla_{b^{2}}L h^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_w2(grad_b2, hidden_layer):\n",
    "    # take grad wrt b^2 and mult by hidden layer\n",
    "    return grad_b2.dot(hidden_layer.T) #(3,1)*(1,d) is (3,d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{h}L = W^{2T} \\nabla_{y^{'}}L = W^{2T} \\nabla_{b^{2}}L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_h(grad_b2, W2):\n",
    "    return np.dot(W2.T, grad_b2) #(d,3)*(3,1) is (d,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta{L}}{\\delta{h^{'}_{i}}} = \\frac{\\delta{L}}{\\delta{h_{i}}} h_{i}(1-h_{i})$ ie multiply each elt in $\\nabla_{h}L$ by $h_{i}(1-h_{i})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_h_tilde(grad_h, h_layer):\n",
    "    # does element wise multiplication\n",
    "    return grad_h * ((h_layer) * (1-h_layer)) # (d,1)(element mult)(d,1) is (d,1)\n",
    "\n",
    "#     vec = []\n",
    "#     for i in range(len(h_layer)):\n",
    "# #         print(\"mult:\", grad_h[i], (h_layer[i] * (1-h_layer[i])))\n",
    "#         vec.append(grad_h[i] * (h_layer[i] * (1-h_layer[i])))\n",
    "\n",
    "#     return np.array(vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{w^{1}} L = (\\nabla_{h^{'}}L)(x^{T})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_w1(grad_h_tilde, input_x):\n",
    "    return np.dot(grad_h_tilde, input_x) #(d,1)*(1,5c) is (d,5c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{b^{1}} L = (\\nabla_{h^{'}}L)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_b1(grad_h_tilde):\n",
    "    return grad_h_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_matrix(filename):\n",
    "    with open(filename, 'r') as handle:\n",
    "        y_final = []\n",
    "#         final_mat = np.zeros(shape=(1,input_dim))\n",
    "        final_mat = []\n",
    "        for line in handle: # for each line\n",
    "            y_test = []\n",
    "\n",
    "\n",
    "            s = line.split()\n",
    "            label = s[0] # Eng, ital, or french\n",
    "            sentence = ' '.join(s[1:]).lower() # rest of sentence\n",
    "\n",
    "            # create (n, 5c) matrix. Each row is a (1, 5c) one hot encoding vector\n",
    "            # n is number of 5 seq characters in the sentence\n",
    "            encode_mat = binarize(seq_chars(sentence))\n",
    "#             final_mat = np.vstack([final_mat, encode_mat])\n",
    "            final_mat.append(encode_mat)\n",
    "            # accumulate pred for each (1,5c) vector\n",
    "            pred = np.zeros(3)\n",
    "\n",
    "            # create arbitrary label for each language\n",
    "            # as long as you're consistent\n",
    "\n",
    "\n",
    "\n",
    "            # for each 5 character encoder vector\n",
    "            for row in range(len(encode_mat)):\n",
    "                    # get that row\n",
    "                if label == \"ENGLISH\":\n",
    "                    y_test.append(1)\n",
    "                elif label == \"ITALIAN\":\n",
    "                    y_test.append(2)\n",
    "                else:\n",
    "                    y_test.append(3)\n",
    "            y_final.append(y_test)\n",
    "#     final_mat = np.delete(final_mat, (0), axis=0)\n",
    "    fin = np.vstack(final_mat)\n",
    "    fin_lab = np.concatenate([np.array(i) for i in y_final])\n",
    "    fin_fin = np.hstack([fin, fin_lab.reshape(len(fin_lab),1)])\n",
    "    return fin_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat, lab = create_input_matrix(filename = \"languageIdentification.data/tiny_train\")\n",
    "# fin = np.vstack(mat)\n",
    "# fin_lab = np.concatenate([np.array(i) for i in lab])\n",
    "# tiny_train_mat = np.hstack([fin, fin_lab.reshape(len(fin_lab),1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start = time.time()\n",
    "# mat, lab = create_input_matrix(filename = \"languageIdentification.data/train\")\n",
    "# end = time.time()\n",
    "# print(end-start)\n",
    "# takes about 5.5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tiny_train_mat.shape)\n",
    "# print(len(mat), len(lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_mat = create_input_matrix(filename = \"languageIdentification.data/train\")\n",
    "\n",
    "# fin = np.vstack(mat)\n",
    "# fin_lab = np.concatenate([np.array(i) for i in lab])\n",
    "\n",
    "# fin_fin = np.hstack([fin, fin_lab.reshape(len(fin_lab),1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin_fin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, W1, bias1, W2, bias2):\n",
    "    # input: \n",
    "        #(5c,1) input vector\n",
    "        # (d,5c) weight matrix W1\n",
    "        # (d,1) bias vector\n",
    "        # (3,d) weight matrix W2\n",
    "        # (3,1) bias vector\n",
    "    # output:\n",
    "        # (d, 1) hidden layer\n",
    "        # (3, 1) predicted vector, y\n",
    "    hidden_layer = sigmoid_array(np.dot(W1, x.T) + bias1)\n",
    "    y = softmax(np.dot(W2, hidden_layer) + bias2)\n",
    "    return hidden_layer, y\n",
    "\n",
    "\n",
    "def backprop(y_pred, y_test, h_layer, input_x, W1, bias1, W2, bias2, eta=.1):   \n",
    "    # input: ingredients for calculating gradients\n",
    "    # output: W1, bias1, W2, bias2 after backprop\n",
    "    \n",
    "    grad_l_y = grad_l_wrt_y(y_pred, y_test)\n",
    "#     print('grad_l_y shape is {}'.format(grad_l_y.shape))\n",
    "#     print(grad_l_y)\n",
    "\n",
    "    grad_l_b2 = grad_l_wrt_b2(y_pred, y_test)\n",
    "#     print(grad_l_b2)\n",
    "#     print('grad_l_b2 shape is {}'.format(grad_l_b2.shape))\n",
    "\n",
    "    grad_l_w2 = grad_l_wrt_w2(grad_l_b2, h_layer)\n",
    "#     print(grad_l_w2)\n",
    "#     print('grad_l_w2 shape is {}'.format(grad_l_w2.shape))\n",
    "\n",
    "    grad_l_h = grad_l_wrt_h(grad_l_b2, W2)\n",
    "#     print(grad_l_h)\n",
    "#     print('grad_l_h shape is {}'.format(grad_l_h.shape))\n",
    "\n",
    "    grad_h_tilde = grad_l_wrt_h_tilde(grad_l_h, h_layer)\n",
    "#     print(grad_h_tilde)\n",
    "#     print('grad_h_tilde shape is {}'.format(grad_h_tilde.shape))\n",
    "\n",
    "    grad_l_w1 = grad_l_wrt_w1(grad_h_tilde, input_x)\n",
    "#     print(grad_l_w1)\n",
    "#     print('grad_l_w1 shape is {}'.format(grad_l_w1.shape))\n",
    "\n",
    "    grad_l_b1 = grad_l_wrt_b1(grad_h_tilde)\n",
    "#     print(grad_l_b1)\n",
    "#     print('grad_l_b1 shape is {}'.format(grad_l_b1.shape))\n",
    "\n",
    "    W1 = W1 - eta * grad_l_w1\n",
    "    W2 = W2 - eta * grad_l_w2\n",
    "    bias2 = bias2 - eta * grad_l_b2\n",
    "    bias1 = bias1 - eta * grad_l_b1\n",
    "\n",
    "    return W1, bias1, W2, bias2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(train_data, W1, bias1, W2, bias2):\n",
    "    \n",
    "    train_labels = train_data[:,-1]\n",
    "    train_data = np.delete(train_data, (-1), axis=1)\n",
    "\n",
    "    # for each 5 character encoder vector\n",
    "    for ind in range(len(train_data)):\n",
    "        # create arbitrary label for each language\n",
    "        if train_labels[ind] == 1: # english\n",
    "            y_test = np.array([0,1,0])\n",
    "        elif train_labels[ind] == 2: # italian\n",
    "            y_test = np.array([1,0,0])\n",
    "        else:\n",
    "            y_test = np.array([0,0,1])\n",
    "\n",
    "\n",
    "        # get that row\n",
    "        input_x = train_data[ind, :].reshape(1,input_dim)\n",
    "        \n",
    "        # forward prop\n",
    "        h_layer, y_pred = forward(input_x, W1, bias1, W2, bias2)\n",
    "        # backwards prop\n",
    "        W1, bias1, W2, bias2 = backprop(y_pred.reshape(3,1), y_test, h_layer, \n",
    "                                        input_x, W1, bias1, W2, bias2, eta=eta)\n",
    "\n",
    "    return W1, bias1, W2, bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_nn(filename, W1, bias1, W2, bias2, file2=None):\n",
    "    loss_li = []\n",
    "    counter = 0\n",
    "    code = 'utf-8'\n",
    "    pred_list = []\n",
    "    # if testing on test set\n",
    "    if file2 is not None:\n",
    "        code = 'latin-1'\n",
    "        with open(file2, 'r') as f:\n",
    "            lines = f.read().lower().splitlines()\n",
    "\n",
    "    with open(filename, 'r', encoding=code) as handle:\n",
    "        num_chances = 0 # number of lines in text file\n",
    "        num_correct = 0 \n",
    "        for line in handle: # for each line\n",
    "            num_chances += 1\n",
    "            if line.split()[0] in ['ENGLISH', 'ITALIAN', 'FRENCH']:\n",
    "                # make sure to do a try except for this for the testing file\n",
    "                s = line.split()\n",
    "                label = s[0].lower() # Eng, ital, or french\n",
    "                sentence = ' '.join(s[1:]).lower() # rest of sentence\n",
    "            else:\n",
    "                sentence = line.lower()\n",
    "                label = lines[counter]\n",
    "                counter += 1\n",
    "\n",
    "            # create (n, 5c) matrix. Each row is a (1, 5c) one hot encoding vector\n",
    "            # n is number of 5 seq characters in the sentence\n",
    "            encode_mat = binarize(seq_chars(sentence))\n",
    "\n",
    "            # accumulate pred for each (1,5c) vector\n",
    "            pred = np.zeros(3)\n",
    "\n",
    "            # create arbitrary label for each language\n",
    "            # as long as you're consistent\n",
    "            if \"english\" in label:\n",
    "                y_test = np.array([0,1,0])\n",
    "            elif 'italian' in label:\n",
    "                y_test = np.array([1,0,0])\n",
    "            else:\n",
    "                y_test = np.array([0,0,1])\n",
    "\n",
    "            num_rows = 0\n",
    "\n",
    "            # for each 5 character encoder vector\n",
    "            for row in range(len(encode_mat)):\n",
    "                # get that row\n",
    "                input_x = encode_mat[row,:].reshape(1,input_dim)\n",
    "                num_rows += 1\n",
    "\n",
    "                # forward prop\n",
    "                h_layer, y_pred = forward(input_x, W1, bias1, W2, bias2)\n",
    "                # accumulate softmax\n",
    "                pred += y_pred.reshape(3,)\n",
    "\n",
    "            # AFTER a single sentence/row is done\n",
    "            pred_avg = pred / num_rows  # avg prediction over all 5 character sequences\n",
    "            loss = np.square(pred_avg - y_test).sum()# calculate loss over pred_avg\n",
    "            loss_li.append(loss)\n",
    "            pred_final = np.zeros(3)\n",
    "            # get index of max probability of pred_avg to make final prediction\n",
    "            ind = np.argmax(pred_avg)\n",
    "            pred_final[ind] = 1\n",
    "            if ind == 1:\n",
    "                rslt = \"English\"\n",
    "            elif ind == 0:\n",
    "                rslt = \"Italian\"\n",
    "            else:\n",
    "                rslt = \"French\"\n",
    "            pred_list.append(rslt)\n",
    "\n",
    "            if np.all(pred_final - y_test == np.array([0,0,0])):\n",
    "                num_correct += 1\n",
    "\n",
    "    # after going through all lines\n",
    "    accuracy = num_correct / num_chances\n",
    "    return accuracy, pred_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "d = 100\n",
    "eta = 0.1\n",
    "\n",
    "W1 = np.random.uniform(size=(d, input_dim))\n",
    "bias1 = np.random.uniform(size=[d,1])\n",
    "W2 = np.random.uniform(size=(3,d))\n",
    "bias2 = np.random.uniform(size=[3, 1])\n",
    "eta = 0.1\n",
    "\n",
    "teeny_mat = create_input_matrix(filename = \"languageIdentification.data/teeny_tiny_train.txt\")\n",
    "# np.random.shuffle(teeny_mat)\n",
    "loss_accum = []\n",
    "# print(teeny_mat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(350):\n",
    "#     np.random.shuffle(teeny_mat)\n",
    "#     W1, bias1, W2, bias2 = train_nn(teeny_mat, W1, bias1, W2, bias2)\n",
    "#     accuracy, _ = test_nn(\"languageIdentification.data/teeny_tiny_train.txt\", W1, bias1, W2, bias2)\n",
    "# #     print(accuracy)\n",
    "# #     plt.plot(np.array(loss_li))\n",
    "# #     plt.show()\n",
    "#     loss_accum.append(loss_li[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_accum)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "d = 100\n",
    "eta = 0.1\n",
    "\n",
    "W1 = np.random.uniform(size=(3, input_dim))\n",
    "bias1 = np.random.uniform(size=[3,1])\n",
    "W2 = np.random.uniform(size=(3,3))\n",
    "bias2 = np.random.uniform(size=[3, 1])\n",
    "eta = 0.1\n",
    "tiny_train_mat = create_input_matrix('languageIdentification.data/tiny_train')\n",
    "# end1 = time.time()\n",
    "\n",
    "\n",
    "acc_li_train = []\n",
    "acc_li_dev = []\n",
    "loss_acum = []\n",
    "# train 3 times\n",
    "for i in range(4):\n",
    "    # get test before training, and after each train\n",
    "    accuracy_t, _ = test_nn(\"languageIdentification.data/tiny_train\", W1, bias1, W2, bias2)\n",
    "    accuracy_d, _ = test_nn(\"languageIdentification.data/dev\", W1, bias1, W2, bias2)\n",
    "\n",
    "\n",
    "    acc_li_train.append(accuracy_t)\n",
    "    acc_li_dev.append(accuracy_d)\n",
    "#     loss_accum.append(loss_li)\n",
    "    # train\n",
    "    np.random.shuffle(tiny_train_mat)\n",
    "    W1, bias1, W2, bias2 = train_nn(tiny_train_mat, W1, bias1, W2, bias2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQwp3EZE49P8Buq/YQUKRkj8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1119080b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.358974358974359, 0.8461538461538461, 0.9641025641025641, 0.9538461538461539]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(acc_li_train)\n",
    "plt.show()\n",
    "acc_li_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111906780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.347, 0.777, 0.949, 0.936]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(acc_li_dev)\n",
    "plt.show()\n",
    "acc_li_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions = test_nn(\"languageIdentification.data/test\", W1, bias1, W2,\n",
    "                        bias2, \"languageIdentification.data/test_solutions\")\n",
    "output_list = []\n",
    "counter = 0\n",
    "outfile = open('languageIdentificationPart1.output', 'w')\n",
    "with open('languageIdentification.data/test', encoding = 'latin-1') as f:\n",
    "    for line in f:\n",
    "        outfile.write(line[:-1] + ' ' + predictions[counter] + '\\n')\n",
    "        counter+=1\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = (250, .75)\n",
    "set2 = (25, .75)\n",
    "set3 = (25, .05)\n",
    "set4 = (100, .05)\n",
    "set5 = (50, .5)\n",
    "\n",
    "params_list = [set1, set2, set3, set4, set5]\n",
    "dict_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tiny_train_mat = create_input_matrix('languageIdentification.data/tiny_train')\n",
    "\n",
    "for params in params_list:\n",
    "    d, eta = params\n",
    "\n",
    "    W1 = np.random.uniform(size=(3, input_dim))\n",
    "    bias1 = np.random.uniform(size=[3,1])\n",
    "    W2 = np.random.uniform(size=(3,3))\n",
    "    bias2 = np.random.uniform(size=[3, 1])\n",
    "    eta = 0.1\n",
    "    # end1 = time.time()\n",
    "\n",
    "\n",
    "#     acc_li_train = []\n",
    "#     acc_li_dev = []\n",
    "#     loss_acum = []\n",
    "    # train 3 times\n",
    "    for i in range(3):\n",
    "        # train\n",
    "        np.random.shuffle(tiny_train_mat)\n",
    "        W1, bias1, W2, bias2 = train_nn(tiny_train_mat, W1, bias1, W2, bias2)\n",
    "\n",
    "\n",
    "    # test on dev\n",
    "    accuracy_d, _ = test_nn(\"languageIdentification.data/dev\", W1, bias1, W2, bias2)\n",
    "    \n",
    "    dict_params[params] = [accuracy_d, W1, bias1, W2, bias2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 0.75) : 0.936\n",
      "(25, 0.75) : 0.948\n",
      "(25, 0.05) : 0.818\n",
      "(100, 0.05) : 0.921\n",
      "(50, 0.5) : 0.762\n",
      "(25, 0.75) [0.936, 0.948, 0.818, 0.921, 0.762]\n"
     ]
    }
   ],
   "source": [
    "li_keys = list(dict_params.keys())\n",
    "best_key = li_keys[0]\n",
    "for key in li_keys:\n",
    "    print(key, \":\", dict_params[key][0])\n",
    "    if dict_params[best_key][0] < dict_params[key][0]:\n",
    "        best_key = key\n",
    "print(best_key, [dict_params[key][0] for key in li_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n"
     ]
    }
   ],
   "source": [
    "W1_best, bias1_best, W2_best, bias2_best = dict_params[best_key][1:]\n",
    "accuracy_test, _ = test_nn(\"languageIdentification.data/test\", W1_best, bias1_best, W2_best, bias2_best,\"languageIdentification.data/test_solutions\")\n",
    "print(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_li = []\n",
    "# loss_acum = []\n",
    "# for i in range(3):\n",
    "#     # get test before training, and after each train\n",
    "#     accuracy, loss_li = test_nn(\"languageIdentification.data/train\", W1, bias1, W2, bias2)\n",
    "#     acc_li.append(accuracy)\n",
    "#     loss_accum.append(loss_li)\n",
    "#     # train\n",
    "\n",
    "#     W1, bias1, W2, bias2 = train_nn(tiny_train_mat, W1, bias1, W2, bias2)\n",
    "# plt.show()\n",
    "# print(sum(loss_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "# d = 100\n",
    "# eta = 0.1\n",
    "\n",
    "# W1 = np.random.uniform(size=(3, input_dim))\n",
    "# bias1 = np.random.uniform(size=[3,1])\n",
    "# W2 = np.random.uniform(size=(3,3))\n",
    "# bias2 = np.random.uniform(size=[3, 1])\n",
    "# eta = 0.1\n",
    "# tiny_train_mat = create_input_matrix('languageIdentification.data/tiny_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(tiny_train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bias2)\n",
    "# W1, bias1, W2, bias2 = train_nn(tiny_train_mat, W1, bias1, W2, bias2)\n",
    "# print(bias2)\n",
    "# accuracy, loss_li = test_nn(\"languageIdentification.data/tiny_train\", W1, bias1, W2, bias2)\n",
    "# print(accuracy)\n",
    "# plt.plot(np.array(loss_li))\n",
    "# plt.show()\n",
    "# print(sum(loss_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dev testing\n",
    "# accuracy, loss_li = test_nn(\"languageIdentification.data/dev\", W1, bias1, W2, bias2)\n",
    "# print(accuracy)\n",
    "# plt.plot(np.array(loss_li))\n",
    "# plt.show()\n",
    "# print(sum(loss_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(tiny_train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bias2)\n",
    "# W1, bias1, W2, bias2 = train_nn(tiny_train_mat, W1, bias1, W2, bias2)\n",
    "# print(bias2)\n",
    "# accuracy, loss_li = test_nn(\"languageIdentification.data/tiny_train\", W1, bias1, W2, bias2)\n",
    "# print(accuracy)\n",
    "# plt.plot(np.array(loss_li))\n",
    "# plt.show()\n",
    "# print(sum(loss_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dev testing\n",
    "# accuracy, loss_li = test_nn(\"languageIdentification.data/dev\", W1, bias1, W2, bias2)\n",
    "# print(accuracy)\n",
    "# print(sum(loss_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.array(loss_li))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test testing\n",
    "# accuracy, loss_li = test_nn(\"languageIdentification.data/test\", W1, bias1, \n",
    "#                             W2, bias2,\"languageIdentification.data/test_solutions\")\n",
    "# print(accuracy)\n",
    "# plt.plot(np.array(loss_li))\n",
    "# plt.show()\n",
    "# print(sum(loss_li))\n",
    "# # end2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .46, .65, .81\n",
    "# dev: .715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .49, .73, .51\n",
    "# dev: .571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2315 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"time to create train matrix\", end1-start)\n",
    "# print('time to finish all training and testing', end2-start) # 40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "# d = 100\n",
    "# eta = 0.1\n",
    "\n",
    "# W1 = np.random.uniform(size=(3, input_dim))\n",
    "# bias1 = np.random.uniform(size=[3,1])\n",
    "# W2 = np.random.uniform(size=(3,3))\n",
    "# bias2 = np.random.uniform(size=[3, 1])\n",
    "# eta = 0.1\n",
    "\n",
    "# teeny_mat = create_input_matrix(filename = \"languageIdentification.data/teeny_tiny_train.txt\")\n",
    "# # np.random.shuffle(teeny_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy, loss_li = test_nn(\"languageIdentification.data/teeny_tiny_train.txt\", W1_new, bias1_new, W2_new, bias2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex = teeny_mat[0,0:415]\n",
    "# h_layer, y_pred = forward(ex, W1, bias1, W2, bias2)\n",
    "# print(h_layer, y_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # h_layer works\n",
    "# a = np.dot(ex, W1.T) + bias1\n",
    "# for val in a:\n",
    "#     print(1/(1+np.exp(-val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = np.dot(W2, h_layer) + bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_pred works\n",
    "# print(np.exp(b) / np.exp(b).sum())\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
