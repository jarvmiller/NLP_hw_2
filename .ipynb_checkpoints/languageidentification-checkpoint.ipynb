{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'ã', 'æ', 'ç', 'è', 'é', 'ê', 'ì', 'í', 'î', 'ï', 'ò', 'ó', 'ô', 'ù', 'ú', 'û', 'ü', 'ÿ', 'œ', ' ', '!', '?', '¿', '¡', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] 94\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_array(x):\n",
    "    # input: array\n",
    "    # output: sigmoid applied to each value of input array\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    # input: array\n",
    "    # output: softmax of array\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def define_alphabet():\n",
    "    # creates list of alphabet to use when parsing text data\n",
    "    base_en = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    special_chars = ' !?¿¡' + string.punctuation + string.digits\n",
    "    italian = 'àèéìíòóùúã'\n",
    "    french = 'àâæçéèêêîïôœùûüÿ'\n",
    "    all_lang_chars = base_en + italian + french \n",
    "    small_chars = list(set(list(all_lang_chars)))\n",
    "    small_chars.sort() \n",
    "    big_chars = list(set(list(all_lang_chars.upper())))\n",
    "    big_chars.sort()\n",
    "    small_chars += special_chars\n",
    "    letters_string = ''\n",
    "    letters = small_chars + big_chars\n",
    "    for letter in letters:\n",
    "        letters_string += letter\n",
    "    return small_chars,big_chars,letters_string\n",
    "\n",
    "alphabet, _, _ = define_alphabet()\n",
    "print(alphabet, len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits labelBinarizer obj to alphabet so it can make one-hot encoding\n",
    "# c = unique characters between eng, fre, ital = 93\n",
    "# le = LabelBinarizer()\n",
    "# le.fit(alphabet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ' '\n",
    "with open(\"languageIdentification.data/train\", 'r') as f:\n",
    "    txt += f.read()\n",
    "# with open(\"languageIdentification.data/dev\", 'r') as f:\n",
    "#     txt += f.read()\n",
    "# with open(\"languageIdentification.data/test\", 'r',encoding = 'latin-1') as f:\n",
    "#     txt += f.read()\n",
    "alphab = list(set(list(txt.lower())))\n",
    "input_dim = (len(alphab)) * 5\n",
    "# print(len(alphab), input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelBinarizer()\n",
    "le.fit(alphab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_chars(s, num_chars=5):\n",
    "    # input: string, s\n",
    "    # output: list of 5 sequential characters of string from beginning to end\n",
    "    n = len(s)\n",
    "    return [s[i:(i+num_chars)] for i in range(n-4)]\n",
    "\n",
    "def binarize(seq_str):\n",
    "    # input: sequence of characters\n",
    "    # output: concatenated one hot encoding of each character\n",
    "        # w/ dimension (5c, 1)\n",
    "    nseq = len(seq_str)\n",
    "    if len(seq_str) == 0:\n",
    "        return np.zeros(shape = (1,input_dim))\n",
    "    for char in seq_str[0]:\n",
    "        if char not in alphab:\n",
    "            print(char, 'not in alphabet')\n",
    "    return np.array([le.transform(list(seq_str[i])) for i in range(nseq)]).reshape(nseq, input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if youve not seen a char before, it's all zeros.. this may be the runtime error\n",
    "# binarize(seq_chars('ㄱㄱabㄱ')).shape\n",
    "# arr = []\n",
    "# for char in 'ㄱㄱabㄱ':\n",
    "#     if char not in alphab:\n",
    "#         arr.append(np.append(le.transform([char])[0], 1))\n",
    "#     else:\n",
    "#         arr.append(np.append(le.transform([char])[0], 0))\n",
    "# # [len(i) for i in arr]\n",
    "# np.concatenate([np.array(i) for i in arr]).reshape(1,input_dim).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{y} L = y - \\hat{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_y(y_pred, y_test):\n",
    "    return y_pred - y_test #(3,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{b^{2}} L = \\nabla_{y^{'}}L = \\sum_{i} \\frac{\\delta{L}}{\\delta{y_{i}}} y_{i}(\\delta{ij} - y_{j}) = \\sum_{i} (y_{i} - \\hat{y_{i}}) y_{i}(\\delta_{ij} - y_{j})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_b2(y_pred, y_test):\n",
    "    vec = []\n",
    "    # for each yj\n",
    "    for j, val in enumerate(list(y_pred)):\n",
    "        counter = 0\n",
    "        # go over all values in yi\n",
    "        for i, val2 in enumerate(list(y_pred)):\n",
    "            if i == j:\n",
    "                counter += (y_pred[i] - y_test[i])*(y_pred[i])*(1-y_pred[j])\n",
    "            else:\n",
    "                counter += (y_pred[i] - y_test[i])*(y_pred[i])*(-y_pred[j])\n",
    "        vec.append(counter)\n",
    "    return np.array(vec)\n",
    "\n",
    "     \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{w^{2}} L = \\nabla_{y^{'}}L h^{T} = \\nabla_{b^{2}}L h^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_w2(grad_b2, hidden_layer):\n",
    "    # take grad wrt b^2 and mult by hidden layer\n",
    "    return grad_b2.dot(hidden_layer.T) #(3,1)*(1,d) is (3,d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{h}L = W^{2T} \\nabla_{y^{'}}L = W^{2T} \\nabla_{b^{2}}L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_h(grad_b2, W2):\n",
    "    return np.dot(W2.T, grad_b2) #(d,3)*(3,1) is (d,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta{L}}{\\delta{h^{'}_{i}}} = \\frac{\\delta{L}}{\\delta{h_{i}}} h_{i}(1-h_{i})$ ie multiply each elt in $\\nabla_{h}L$ by $h_{i}(1-h_{i})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_h_tilde(grad_h, h_layer):\n",
    "    # does element wise multiplication\n",
    "    return grad_h * ((h_layer) * (1-h_layer)) # (d,1)(element mult)(d,1) is (d,1)\n",
    "\n",
    "#     vec = []\n",
    "#     for i in range(len(h_layer)):\n",
    "# #         print(\"mult:\", grad_h[i], (h_layer[i] * (1-h_layer[i])))\n",
    "#         vec.append(grad_h[i] * (h_layer[i] * (1-h_layer[i])))\n",
    "\n",
    "#     return np.array(vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{w^{1}} L = (\\nabla_{h^{'}}L)(x^{T})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_w1(grad_h_tilde, input_x):\n",
    "    return np.dot(grad_h_tilde, input_x) #(d,1)*(1,5c) is (d,5c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{b^{1}} L = (\\nabla_{h^{'}}L)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_l_wrt_b1(grad_h_tilde):\n",
    "    return grad_h_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_matrix(filename):\n",
    "    with open(filename, 'r') as handle:\n",
    "        y_final = []\n",
    "#         final_mat = np.zeros(shape=(1,input_dim))\n",
    "        final_mat = []\n",
    "        for line in handle: # for each line\n",
    "            y_test = []\n",
    "\n",
    "\n",
    "            s = line.split()\n",
    "            label = s[0] # Eng, ital, or french\n",
    "            sentence = ' '.join(s[1:]).lower() # rest of sentence\n",
    "\n",
    "            # create (n, 5c) matrix. Each row is a (1, 5c) one hot encoding vector\n",
    "            # n is number of 5 seq characters in the sentence\n",
    "            encode_mat = binarize(seq_chars(sentence))\n",
    "#             final_mat = np.vstack([final_mat, encode_mat])\n",
    "            final_mat.append(encode_mat)\n",
    "            # accumulate pred for each (1,5c) vector\n",
    "            pred = np.zeros(3)\n",
    "\n",
    "            # create arbitrary label for each language\n",
    "            # as long as you're consistent\n",
    "\n",
    "\n",
    "\n",
    "            # for each 5 character encoder vector\n",
    "            for row in range(len(encode_mat)):\n",
    "                    # get that row\n",
    "                if label == \"ENGLISH\":\n",
    "                    y_test.append(1)\n",
    "                elif label == \"ITALIAN\":\n",
    "                    y_test.append(2)\n",
    "                else:\n",
    "                    y_test.append(3)\n",
    "            y_final.append(y_test)\n",
    "#     final_mat = np.delete(final_mat, (0), axis=0)\n",
    "    fin = np.vstack(final_mat)\n",
    "    fin_lab = np.concatenate([np.array(i) for i in y_final])\n",
    "    fin_fin = np.hstack([fin, fin_lab.reshape(len(fin_lab),1)])\n",
    "    return fin_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, W1, bias1, W2, bias2):\n",
    "    # input: \n",
    "        #(5c,1) input vector\n",
    "        # (d,5c) weight matrix W1\n",
    "        # (d,1) bias vector\n",
    "        # (3,d) weight matrix W2\n",
    "        # (3,1) bias vector\n",
    "    # output:\n",
    "        # (d, 1) hidden layer\n",
    "        # (3, 1) predicted vector, y\n",
    "    hidden_layer = sigmoid_array(np.dot(W1, x.T) + bias1)\n",
    "    y = softmax(np.dot(W2, hidden_layer) + bias2)\n",
    "    return hidden_layer, y\n",
    "\n",
    "\n",
    "def backprop(y_pred, y_test, h_layer, input_x, W1, bias1, W2, bias2, eta=.1):   \n",
    "    # input: ingredients for calculating gradients\n",
    "    # output: W1, bias1, W2, bias2 after backprop\n",
    "    \n",
    "    grad_l_y = grad_l_wrt_y(y_pred, y_test)\n",
    "#     print('grad_l_y shape is {}'.format(grad_l_y.shape))\n",
    "#     print(grad_l_y)\n",
    "\n",
    "    grad_l_b2 = grad_l_wrt_b2(y_pred, y_test)\n",
    "#     print(grad_l_b2)\n",
    "#     print('grad_l_b2 shape is {}'.format(grad_l_b2.shape))\n",
    "\n",
    "    grad_l_w2 = grad_l_wrt_w2(grad_l_b2, h_layer)\n",
    "#     print(grad_l_w2)\n",
    "#     print('grad_l_w2 shape is {}'.format(grad_l_w2.shape))\n",
    "\n",
    "    grad_l_h = grad_l_wrt_h(grad_l_b2, W2)\n",
    "#     print(grad_l_h)\n",
    "#     print('grad_l_h shape is {}'.format(grad_l_h.shape))\n",
    "\n",
    "    grad_h_tilde = grad_l_wrt_h_tilde(grad_l_h, h_layer)\n",
    "#     print(grad_h_tilde)\n",
    "#     print('grad_h_tilde shape is {}'.format(grad_h_tilde.shape))\n",
    "\n",
    "    grad_l_w1 = grad_l_wrt_w1(grad_h_tilde, input_x)\n",
    "#     print(grad_l_w1)\n",
    "#     print('grad_l_w1 shape is {}'.format(grad_l_w1.shape))\n",
    "\n",
    "    grad_l_b1 = grad_l_wrt_b1(grad_h_tilde)\n",
    "#     print(grad_l_b1)\n",
    "#     print('grad_l_b1 shape is {}'.format(grad_l_b1.shape))\n",
    "\n",
    "    W1 = W1 - eta * grad_l_w1\n",
    "    W2 = W2 - eta * grad_l_w2\n",
    "    bias2 = bias2 - eta * grad_l_b2\n",
    "    bias1 = bias1 - eta * grad_l_b1\n",
    "\n",
    "    return W1, bias1, W2, bias2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(train_data, W1, bias1, W2, bias2):\n",
    "    \n",
    "    train_labels = train_data[:,-1]\n",
    "    train_data = np.delete(train_data, (-1), axis=1)\n",
    "\n",
    "    # for each 5 character encoder vector\n",
    "    for ind in range(len(train_data)):\n",
    "        # create arbitrary label for each language\n",
    "        if train_labels[ind] == 1: # english\n",
    "            y_test = np.array([0,1,0])\n",
    "        elif train_labels[ind] == 2: # italian\n",
    "            y_test = np.array([1,0,0])\n",
    "        else:\n",
    "            y_test = np.array([0,0,1])\n",
    "\n",
    "\n",
    "        # get that row\n",
    "        input_x = train_data[ind, :].reshape(1,input_dim)\n",
    "        \n",
    "        # forward prop\n",
    "        h_layer, y_pred = forward(input_x, W1, bias1, W2, bias2)\n",
    "        # backwards prop\n",
    "        W1, bias1, W2, bias2 = backprop(y_pred.reshape(3,1), y_test, h_layer, \n",
    "                                        input_x, W1, bias1, W2, bias2, eta=eta)\n",
    "\n",
    "    return W1, bias1, W2, bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_nn(filename, W1, bias1, W2, bias2, file2=None):\n",
    "    loss_li = []\n",
    "    counter = 0\n",
    "    code = 'utf-8'\n",
    "    pred_list = []\n",
    "    # if testing on test set\n",
    "    if file2 is not None:\n",
    "        code = 'latin-1'\n",
    "        with open(file2, 'r') as f:\n",
    "            lines = f.read().lower().splitlines()\n",
    "\n",
    "    with open(filename, 'r', encoding=code) as handle:\n",
    "        num_chances = 0 # number of lines in text file\n",
    "        num_correct = 0 \n",
    "        for line in handle: # for each line\n",
    "            num_chances += 1\n",
    "            if line.split()[0] in ['ENGLISH', 'ITALIAN', 'FRENCH']:\n",
    "                # make sure to do a try except for this for the testing file\n",
    "                s = line.split()\n",
    "                label = s[0].lower() # Eng, ital, or french\n",
    "                sentence = ' '.join(s[1:]).lower() # rest of sentence\n",
    "            else:\n",
    "                sentence = line.lower()\n",
    "                label = lines[counter]\n",
    "                counter += 1\n",
    "\n",
    "            # create (n, 5c) matrix. Each row is a (1, 5c) one hot encoding vector\n",
    "            # n is number of 5 seq characters in the sentence\n",
    "            encode_mat = binarize(seq_chars(sentence))\n",
    "\n",
    "            # accumulate pred for each (1,5c) vector\n",
    "            pred = np.zeros(3)\n",
    "\n",
    "            # create arbitrary label for each language\n",
    "            # as long as you're consistent\n",
    "            if \"english\" in label:\n",
    "                y_test = np.array([0,1,0])\n",
    "            elif 'italian' in label:\n",
    "                y_test = np.array([1,0,0])\n",
    "            else:\n",
    "                y_test = np.array([0,0,1])\n",
    "\n",
    "            num_rows = 0\n",
    "\n",
    "            # for each 5 character encoder vector\n",
    "            for row in range(len(encode_mat)):\n",
    "                # get that row\n",
    "                input_x = encode_mat[row,:].reshape(1,input_dim)\n",
    "                num_rows += 1\n",
    "\n",
    "                # forward prop\n",
    "                h_layer, y_pred = forward(input_x, W1, bias1, W2, bias2)\n",
    "                # accumulate softmax\n",
    "                pred += y_pred.reshape(3,)\n",
    "\n",
    "            # AFTER a single sentence/row is done\n",
    "            pred_avg = pred / num_rows  # avg prediction over all 5 character sequences\n",
    "            loss = np.square(pred_avg - y_test).sum()# calculate loss over pred_avg\n",
    "            loss_li.append(loss)\n",
    "            pred_final = np.zeros(3)\n",
    "            # get index of max probability of pred_avg to make final prediction\n",
    "            ind = np.argmax(pred_avg)\n",
    "            pred_final[ind] = 1\n",
    "            if ind == 1:\n",
    "                rslt = \"English\"\n",
    "            elif ind == 0:\n",
    "                rslt = \"Italian\"\n",
    "            else:\n",
    "                rslt = \"French\"\n",
    "            pred_list.append(rslt)\n",
    "\n",
    "            if np.all(pred_final - y_test == np.array([0,0,0])):\n",
    "                num_correct += 1\n",
    "\n",
    "    # after going through all lines\n",
    "    accuracy = num_correct / num_chances\n",
    "    return accuracy, pred_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "d = 100\n",
    "eta = 0.1\n",
    "\n",
    "W1 = np.random.uniform(size=(d, input_dim))\n",
    "bias1 = np.random.uniform(size=[d,1])\n",
    "W2 = np.random.uniform(size=(3,d))\n",
    "bias2 = np.random.uniform(size=[3, 1])\n",
    "eta = 0.1\n",
    "\n",
    "teeny_mat = create_input_matrix(filename = \"languageIdentification.data/teeny_tiny_train.txt\")\n",
    "# np.random.shuffle(teeny_mat)\n",
    "loss_accum = []\n",
    "# print(teeny_mat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(350):\n",
    "#     np.random.shuffle(teeny_mat)\n",
    "#     W1, bias1, W2, bias2 = train_nn(teeny_mat, W1, bias1, W2, bias2)\n",
    "#     accuracy, _ = test_nn(\"languageIdentification.data/teeny_tiny_train.txt\", W1, bias1, W2, bias2)\n",
    "# #     print(accuracy)\n",
    "# #     plt.plot(np.array(loss_li))\n",
    "# #     plt.show()\n",
    "#     loss_accum.append(loss_li[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_accum)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "d = 100\n",
    "eta = 0.1\n",
    "\n",
    "W1 = np.random.uniform(size=(d, input_dim))\n",
    "bias1 = np.random.uniform(size=[d,1])\n",
    "W2 = np.random.uniform(size=(3,d))\n",
    "bias2 = np.random.uniform(size=[3, 1])\n",
    "eta = 0.1\n",
    "tiny_train_mat = create_input_matrix('languageIdentification.data/tiny_train')\n",
    "# end1 = time.time()\n",
    "\n",
    "\n",
    "acc_li_train = []\n",
    "acc_li_dev = []\n",
    "loss_acum = []\n",
    "# train 3 times\n",
    "for i in range(4):\n",
    "    # get test before training, and after each train\n",
    "    accuracy_t, _ = test_nn(\"languageIdentification.data/tiny_train\", W1, bias1, W2, bias2)\n",
    "    accuracy_d, _ = test_nn(\"languageIdentification.data/dev\", W1, bias1, W2, bias2)\n",
    "\n",
    "\n",
    "    acc_li_train.append(accuracy_t)\n",
    "    acc_li_dev.append(accuracy_d)\n",
    "#     loss_accum.append(loss_li)\n",
    "    # train\n",
    "    np.random.shuffle(tiny_train_mat)\n",
    "    W1, bias1, W2, bias2 = train_nn(tiny_train_mat, W1, bias1, W2, bias2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAESCAYAAADTx4MfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1195e9780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(acc_li_train)\n",
    "plt.plot(acc_li_dev)\n",
    "plt.legend(['train accuracy', 'dev accuracy'], loc='upper left')\n",
    "plt.xlabel('epoch', fontsize=18)\n",
    "plt.ylabel('accuracy', fontsize=16)\n",
    "plt.xticks([0,1,2,3])\n",
    "plt.show()\n",
    "fig.savefig('accuracy.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions = test_nn(\"languageIdentification.data/test\", W1, bias1, W2,\n",
    "                        bias2, \"languageIdentification.data/test_solutions\")\n",
    "output_list = []\n",
    "counter = 0\n",
    "outfile = open('languageIdentificationPart1.output', 'w')\n",
    "with open('languageIdentification.data/test', encoding = 'latin-1') as f:\n",
    "    for line in f:\n",
    "        outfile.write(line[:-1] + ' ' + predictions[counter] + '\\n')\n",
    "        counter+=1\n",
    "outfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = (250, .75)\n",
    "set2 = (25, .75)\n",
    "set3 = (25, .05)\n",
    "set4 = (100, .05)\n",
    "set5 = (3, .075)\n",
    "\n",
    "params_list = [set1, set2, set3, set4, set5]\n",
    "dict_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "tiny_train_mat = create_input_matrix('languageIdentification.data/tiny_train')\n",
    "\n",
    "for params in params_list:\n",
    "    d, eta = params\n",
    "\n",
    "    W1 = np.random.uniform(size=(d, input_dim))\n",
    "    bias1 = np.random.uniform(size=[d,1])\n",
    "    W2 = np.random.uniform(size=(3,d))\n",
    "    bias2 = np.random.uniform(size=[3, 1])\n",
    "    eta = 0.1\n",
    "    # end1 = time.time()\n",
    "\n",
    "\n",
    "#     acc_li_train = []\n",
    "#     acc_li_dev = []\n",
    "#     loss_acum = []\n",
    "    # train 3 times\n",
    "    for i in range(3):\n",
    "        # train\n",
    "        np.random.shuffle(tiny_train_mat)\n",
    "        W1, bias1, W2, bias2 = train_nn(tiny_train_mat, W1, bias1, W2, bias2)\n",
    "\n",
    "\n",
    "    # test on dev\n",
    "    accuracy_d, _ = test_nn(\"languageIdentification.data/dev\", W1, bias1, W2, bias2)\n",
    "    \n",
    "    dict_params[params] = [accuracy_d, W1, bias1, W2, bias2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 0.75) : 0.347\n",
      "(25, 0.75) : 0.687\n",
      "(25, 0.05) : 0.984\n",
      "(100, 0.05) : 0.381\n",
      "(3, 0.075) : 0.838\n",
      "(25, 0.05) [0.347, 0.687, 0.984, 0.381, 0.838]\n"
     ]
    }
   ],
   "source": [
    "li_keys = list(dict_params.keys())\n",
    "best_key = li_keys[0]\n",
    "for key in li_keys:\n",
    "    print(key, \":\", dict_params[key][0])\n",
    "    if dict_params[best_key][0] < dict_params[key][0]:\n",
    "        best_key = key\n",
    "print(best_key, [dict_params[key][0] for key in li_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "W1_best, bias1_best, W2_best, bias2_best = dict_params[best_key][1:]\n",
    "accuracy_test, _ = test_nn(\"languageIdentification.data/test\", W1_best, bias1_best, W2_best, bias2_best,\"languageIdentification.data/test_solutions\")\n",
    "print(accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_li = []\n",
    "# loss_acum = []\n",
    "# for i in range(3):\n",
    "#     # get test before training, and after each train\n",
    "#     accuracy, loss_li = test_nn(\"languageIdentification.data/train\", W1, bias1, W2, bias2)\n",
    "#     acc_li.append(accuracy)\n",
    "#     loss_accum.append(loss_li)\n",
    "#     # train\n",
    "\n",
    "#     W1, bias1, W2, bias2 = train_nn(tiny_train_mat, W1, bias1, W2, bias2)\n",
    "# plt.show()\n",
    "# print(sum(loss_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "# d = 100\n",
    "# eta = 0.1\n",
    "\n",
    "# W1 = np.random.uniform(size=(3, input_dim))\n",
    "# bias1 = np.random.uniform(size=[3,1])\n",
    "# W2 = np.random.uniform(size=(3,3))\n",
    "# bias2 = np.random.uniform(size=[3, 1])\n",
    "# eta = 0.1\n",
    "# tiny_train_mat = create_input_matrix('languageIdentification.data/tiny_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(tiny_train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bias2)\n",
    "# W1, bias1, W2, bias2 = train_nn(tiny_train_mat, W1, bias1, W2, bias2)\n",
    "# print(bias2)\n",
    "# accuracy, loss_li = test_nn(\"languageIdentification.data/tiny_train\", W1, bias1, W2, bias2)\n",
    "# print(accuracy)\n",
    "# plt.plot(np.array(loss_li))\n",
    "# plt.show()\n",
    "# print(sum(loss_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dev testing\n",
    "# accuracy, loss_li = test_nn(\"languageIdentification.data/dev\", W1, bias1, W2, bias2)\n",
    "# print(accuracy)\n",
    "# plt.plot(np.array(loss_li))\n",
    "# plt.show()\n",
    "# print(sum(loss_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(tiny_train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bias2)\n",
    "# W1, bias1, W2, bias2 = train_nn(tiny_train_mat, W1, bias1, W2, bias2)\n",
    "# print(bias2)\n",
    "# accuracy, loss_li = test_nn(\"languageIdentification.data/tiny_train\", W1, bias1, W2, bias2)\n",
    "# print(accuracy)\n",
    "# plt.plot(np.array(loss_li))\n",
    "# plt.show()\n",
    "# print(sum(loss_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dev testing\n",
    "# accuracy, loss_li = test_nn(\"languageIdentification.data/dev\", W1, bias1, W2, bias2)\n",
    "# print(accuracy)\n",
    "# print(sum(loss_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.array(loss_li))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test testing\n",
    "# accuracy, loss_li = test_nn(\"languageIdentification.data/test\", W1, bias1, \n",
    "#                             W2, bias2,\"languageIdentification.data/test_solutions\")\n",
    "# print(accuracy)\n",
    "# plt.plot(np.array(loss_li))\n",
    "# plt.show()\n",
    "# print(sum(loss_li))\n",
    "# # end2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .46, .65, .81\n",
    "# dev: .715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .49, .73, .51\n",
    "# dev: .571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2315 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"time to create train matrix\", end1-start)\n",
    "# print('time to finish all training and testing', end2-start) # 40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "# d = 100\n",
    "# eta = 0.1\n",
    "\n",
    "# W1 = np.random.uniform(size=(3, input_dim))\n",
    "# bias1 = np.random.uniform(size=[3,1])\n",
    "# W2 = np.random.uniform(size=(3,3))\n",
    "# bias2 = np.random.uniform(size=[3, 1])\n",
    "# eta = 0.1\n",
    "\n",
    "# teeny_mat = create_input_matrix(filename = \"languageIdentification.data/teeny_tiny_train.txt\")\n",
    "# # np.random.shuffle(teeny_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy, loss_li = test_nn(\"languageIdentification.data/teeny_tiny_train.txt\", W1_new, bias1_new, W2_new, bias2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex = teeny_mat[0,0:415]\n",
    "# h_layer, y_pred = forward(ex, W1, bias1, W2, bias2)\n",
    "# print(h_layer, y_pred.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # h_layer works\n",
    "# a = np.dot(ex, W1.T) + bias1\n",
    "# for val in a:\n",
    "#     print(1/(1+np.exp(-val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = np.dot(W2, h_layer) + bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_pred works\n",
    "# print(np.exp(b) / np.exp(b).sum())\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
