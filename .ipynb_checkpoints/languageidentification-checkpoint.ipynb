{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import string\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'æ', 'ç', 'è', 'é', 'ê', 'ì', 'í', 'î', 'ï', 'ò', 'ó', 'ô', 'ù', 'ú', 'û', 'ü', 'ÿ', 'œ', ' ', '!', '?', '¿', '¡', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] 93\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_array(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "\n",
    "def define_alphabet():\n",
    "    base_en = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    special_chars = ' !?¿¡' + string.punctuation + string.digits\n",
    "    italian = 'àèéìíòóùú'\n",
    "    french = 'àâæçéèêêîïôœùûüÿ'\n",
    "    all_lang_chars = base_en + italian + french \n",
    "    small_chars = list(set(list(all_lang_chars)))\n",
    "    small_chars.sort() \n",
    "    big_chars = list(set(list(all_lang_chars.upper())))\n",
    "    big_chars.sort()\n",
    "    small_chars += special_chars\n",
    "    letters_string = ''\n",
    "    letters = small_chars + big_chars\n",
    "    for letter in letters:\n",
    "        letters_string += letter\n",
    "    return small_chars,big_chars,letters_string\n",
    "\n",
    "alphabet, _, _ = define_alphabet()\n",
    "print(alphabet, len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of an input 'x' which is 5c,1\n",
    "# c = unique characters between eng, fre, ital = 93\n",
    "le = LabelBinarizer()\n",
    "le.fit(alphabet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "# d = 100\n",
    "# eta = 0.1\n",
    "# input_dim = 455\n",
    "\n",
    "# W1 = np.random.uniform(size=(d, input_dim))\n",
    "# bias1 = np.random.uniform(size=[d,1])\n",
    "# W2 = np.random.uniform(size=(3,d))\n",
    "# bias2 = np.random.uniform(size=[3, 1])\n",
    "\n",
    "def forward(x, W1, bias1, W2, bias2):\n",
    "    hidden_layer = sigmoid_array(np.dot(W1, x.T) + bias1)\n",
    "    y = softmax(np.dot(W2, hidden_layer) + bias2)\n",
    "    return hidden_layer, y\n",
    "\n",
    "# h_layer, y_pred = forward(x, W1, bias1, W2, bias2)\n",
    "# y_test = np.array([1,0, 0]).reshape(3,1)\n",
    "# print(W1.shape, bias1.shape, W2.shape, bias2.shape,\n",
    "#       h_layer.shape, y_pred.shape, y_test.shape)\n",
    "\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions so far\n",
    "- how to know which index in softmax corresponds to which language?\n",
    "- \"Shuffle   your   training   data   to   prevent   the   network   overfitting   to   one   output   class.\"??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_chars(s, num_chars=5):\n",
    "    n = len(s)\n",
    "    return [s[i:(i+num_chars)] for i in range(n-4)]\n",
    "\n",
    "def binarize(seq_str):\n",
    "    nseq = len(seq_str)\n",
    "    return np.array([le.transform(list(seq_str[i])) for i in range(nseq)]).reshape(nseq, input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for backpropogation\n",
    "def grad_l_wrt_y(y_pred, y_true):\n",
    "    return y_pred - y_true #(3,1)\n",
    "\n",
    "def grad_l_wrt_b2(y_pred, y_true):\n",
    "    vec = []\n",
    "    for j, val in enumerate(list(y_pred)):\n",
    "        counter = 0\n",
    "        for i, val2 in enumerate(list(y_pred)):\n",
    "            if i == j:\n",
    "                counter += (y_pred[i] - y_test[i])*(y_pred[i])*(1-y_pred[j])\n",
    "            else:\n",
    "                counter += (y_pred[i] - y_test[i])*(y_pred[i])*(-y_pred[j])\n",
    "        vec.append(counter)\n",
    "    return np.array(vec)\n",
    "\n",
    "def grad_l_wrt_w2(grad_b2, hidden_layer):\n",
    "    return grad_b2.dot(hidden_layer.T) #(3,1)*(1,d) is (3,d)\n",
    "\n",
    "def grad_l_wrt_h(grad_b2, W2):\n",
    "    return np.dot(W2.T, grad_b2) #(d,3)*(3,1) is (d,1)\n",
    "\n",
    "def grad_l_wrt_h_tilde(grad_h, h_layer):\n",
    "    return grad_h * ((h_layer) * (1-h_layer)) # (d,1)(elt *)(d,1) is (d,1)\n",
    "#     vec = []\n",
    "#     for i in range(len(h_layer)):\n",
    "# #         print(\"mult:\", grad_h[i], (h_layer[i] * (1-h_layer[i])))\n",
    "#         vec.append(grad_h[i] * (h_layer[i] * (1-h_layer[i])))\n",
    "#     return np.array(vec)\n",
    "\n",
    "def grad_l_wrt_w1(grad_h_tilde, input_x):\n",
    "#     x = input_x.reshape([455,])\n",
    "#     h = grad_h_tilde.reshape([100,])\n",
    "    \n",
    "#     _, a = np.nonzero(input_x)\n",
    "#     b = np.where(grad_h_tilde == 0)\n",
    "#     print(a, b)\n",
    "#     print('nonzero vals', x[a])\n",
    "#     print('nonzero?', np.dot(grad_h_tilde, input_x)[np.nonzero(np.dot(grad_h_tilde, input_x))])\n",
    "#     return np.outer(grad_h_tilde.reshape([100,]), input_x.reshape([455,]))\n",
    "    return np.dot(grad_h_tilde, input_x) #(d,1)*(1,5c) is (d,5c)\n",
    "\n",
    "def grad_l_wrt_b1(grad_h_tilde):\n",
    "    return grad_h_tilde\n",
    "\n",
    "def backprop(y_pred, y_test, h_layer, input_x, W1, W2, bias1, bias2, eta=.1):   \n",
    "\n",
    "    grad_l_y = grad_l_wrt_y(y_pred, y_test)\n",
    "#     print('grad_l_y shape is {}'.format(grad_l_y.shape))\n",
    "#     print(grad_l_y)\n",
    "    grad_l_b2 = grad_l_wrt_b2(y_pred, y_test)\n",
    "#     print(grad_l_b2)\n",
    "#     print('grad_l_b2 shape is {}'.format(grad_l_b2.shape))\n",
    "    grad_l_w2 = grad_l_wrt_w2(grad_l_b2, h_layer)\n",
    "#     print(grad_l_w2)\n",
    "#     print('grad_l_w2 shape is {}'.format(grad_l_w2.shape))\n",
    "    grad_l_h = grad_l_wrt_h(grad_l_b2, W2)\n",
    "#     print(grad_l_h)\n",
    "#     print('grad_l_h shape is {}'.format(grad_l_h.shape))\n",
    "    grad_h_tilde = grad_l_wrt_h_tilde(grad_l_h, h_layer)\n",
    "#     print(grad_h_tilde)\n",
    "#     print('grad_h_tilde shape is {}'.format(grad_h_tilde.shape))\n",
    "    grad_l_w1 = grad_l_wrt_w1(grad_h_tilde, input_x)\n",
    "#     print(grad_l_w1)\n",
    "#     print('grad_l_w1 shape is {}'.format(grad_l_w1.shape))\n",
    "    grad_l_b1 = grad_l_wrt_b1(grad_h_tilde)\n",
    "#     print(grad_l_b1)\n",
    "#     print('grad_l_b1 shape is {}'.format(grad_l_b1.shape))\n",
    "\n",
    "    # no error\n",
    "#     print(np.nonzero(W1))\n",
    "#     print(W1[np.nonzero(W1)], W1.shape)\n",
    "    # confirms that there are many nonzero, but mostly 0, and that eta * grad works\n",
    "#     print(grad_l_w1[np.nonzero(grad_l_w1)], (eta * grad_l_w1)[np.nonzero(eta*grad_l_w1)])\n",
    "    W1 = W1 -  eta * grad_l_w1\n",
    "    # this shows all rows exaclty the same except for 5, which corresponds to the 5 \"1\" values \n",
    "    # that were in the input\n",
    "#     print(sum(sum(W1t==W1)))\n",
    "    W2 = W2 - eta * grad_l_w2\n",
    "    bias2 = bias2 - eta * grad_l_b2\n",
    "    bias1 = bias1 - eta * grad_l_b1\n",
    "#     print(np.nonzero(grad_l_w1))\n",
    "#     print(grad_l_w1[np.nonzero(grad_l_w1)], grad_l_w1.shape)\n",
    "#     print(np.nonzero(W1))\n",
    "#     print(W1[np.nonzero(W1)], W1.shape)\n",
    "    return W1, bias1, W2, bias2\n",
    "\n",
    "\n",
    "# print('before backprop', W1, bias1, W2, bias2)\n",
    "# print('post backprop', backprop(y_pred, y_test, h_layer, x, W1, W2,\n",
    "#                                bias1, bias2))\n",
    "\n",
    "# w1t, b1t, w2t, b2t = backprop(y_pred, y_test, h_layer, x_test, W1, W2, bias1, bias2, eta)\n",
    "\n",
    "\n",
    "# # # these are exactly equal sadly...\n",
    "# sum(sum(w1t!=W1))\n",
    "# # np.nonzero(w1t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.89258636]\n",
      " [ 0.73800169]\n",
      " [ 0.70865164]]\n",
      "[[ 0.89347588]\n",
      " [ 0.7376984 ]\n",
      " [ 0.70806541]]\n",
      "[[ 0.89444527]\n",
      " [ 0.73739031]\n",
      " [ 0.70740412]]\n",
      "[0.8615384615384616, 0.8615384615384616, 0.8615384615384616]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "d = 100\n",
    "eta = 0.1\n",
    "input_dim = 455\n",
    "\n",
    "W1 = np.random.uniform(size=(d, input_dim))\n",
    "bias1 = np.random.uniform(size=[d,1])\n",
    "W2 = np.random.uniform(size=(3,d))\n",
    "bias2 = np.random.uniform(size=[3, 1])\n",
    "filename = \"languageIdentification.data/tiny_train\"\n",
    "li = []\n",
    "for i in range(3):\n",
    "    with open(filename, 'r') as handle:\n",
    "        # init weights and biases\n",
    "\n",
    "    #     eta = 0.1\n",
    "        num_chances = 0\n",
    "        num_correct = 0\n",
    "        for line in handle:\n",
    "            num_chances += 1\n",
    "            s = line.split()\n",
    "            label = s[0]\n",
    "            sentence = ' '.join(s[1:]).lower()\n",
    "            encode_mat = binarize(seq_chars(sentence))\n",
    "    #         print(np.nonzero(encode_mat)[1])\n",
    "    #         print(\"next line\")\n",
    "            # accumulate pred for each line, take avg, then get error\n",
    "            pred = np.zeros(3)\n",
    "\n",
    "            if label == \"ENGLISH\":\n",
    "                y_test = np.array([0,1,0])\n",
    "            elif label == \"ITALIAN\":\n",
    "                y_test = np.array([1,0,0])\n",
    "            else:\n",
    "                y_test = np.array([0,0,1])\n",
    "\n",
    "    #         # for each 5 character encoder vector\n",
    "            num_rows = 0\n",
    "            for row in range(len(encode_mat)):\n",
    "                input_x = encode_mat[row,:].reshape(1,455)\n",
    "                num_rows += 1\n",
    "    #             # forward prop\n",
    "                h_layer, y_pred = forward(input_x, W1, bias1, W2, bias2)\n",
    "    #             # accumulate softmax\n",
    "                pred += y_pred.reshape(3,)\n",
    "    #             print(pred)\n",
    "    #             # calculate l_2 loss\n",
    "    #             loss = np.square(y_pred-y_test).sum()\n",
    "    #             print(loss)\n",
    "    # #             # update via SGD and backpropogation\n",
    "    #             W1, bias1, W2, bias2 = backprop(y_pred, y_test, h_layer, \n",
    "    #                                             input_x, W1, W2, bias1, bias2, eta=eta)     \n",
    "\n",
    "    # #         # DO THIS 3 TIMES for testing. then use W1, bias1, \n",
    "    # #         # W2, bias2 on testing data\n",
    "    # #         # THIS WOULD BE DONE WHEN TESTING\n",
    "    # #         # after a sentence is done\n",
    "            pred_avg = pred / num_rows  # avg softmax over each 5 letter pred of sentence\n",
    "    #         loss = np.square(pred_avg-y_test).sum()\n",
    "    #         if loss > .3:\n",
    "    #             print(loss, pred_avg, y_test, line)\n",
    "    #         print('sentence done', pred, num_rows, pred_avg)\n",
    "            pred_final = np.zeros(3)\n",
    "    # #         # index of max prob is chosen to be one\n",
    "            pred_final[np.argmax(pred_avg)] = 1\n",
    "    #         print(pred_final, y_test)\n",
    "    # #         # if correct guess\n",
    "            if np.all(pred_final - y_test == np.array([0,0,0])):\n",
    "                num_correct += 1\n",
    "            # im guessing we should backprop after each sentence, not after each 5 character thingy\n",
    "            W1, bias1, W2, bias2 = backprop(pred_avg.reshape(3,1), y_test, h_layer, \n",
    "                                                input_x, W1, W2, bias1, bias2, eta=eta) \n",
    "\n",
    "    # #     # after going through all lines\n",
    "    accuracy = num_correct / num_chances\n",
    "    # I can see that parameters are changing... like bias2. but accuracy the same lol\n",
    "    print(bias2)\n",
    "    li.append(accuracy)\n",
    "print(li) # hmm this should be getting better\n",
    "# the problem is that I predict [1,0,0] all the time if i dont train....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example of an input 'x' which is 5c,1\n",
    "# # c = unique characters between eng, fre, ital = 93\n",
    "# le = LabelEncoder()\n",
    "# oe = OneHotEncoder(sparse=False)\n",
    "# int_encoded = le.fit_transform(np.array(alphabet))\n",
    "# int_encoded = int_encoded.reshape(len(int_encoded),1)\n",
    "# # s = list('ggéré'.lower())\n",
    "# # x = le.transform(s)\n",
    "# x = oe.fit_transform(int_encoded)\n",
    "# # print(x, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
