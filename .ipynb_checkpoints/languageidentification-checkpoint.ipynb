{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import string\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'æ', 'ç', 'è', 'é', 'ê', 'ì', 'í', 'î', 'ï', 'ò', 'ó', 'ô', 'ù', 'ú', 'û', 'ü', 'ÿ', 'œ', ' ', '!', '?', '¿', '¡', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] 93\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_array(x):\n",
    "    # input: array\n",
    "    # output: sigmoid applied to each value of input array\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    # input: array\n",
    "    # output: softmax of array\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def define_alphabet():\n",
    "    # creates list of alphabet to use when parsing text data\n",
    "    base_en = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    special_chars = ' !?¿¡' + string.punctuation + string.digits\n",
    "    italian = 'àèéìíòóùú'\n",
    "    french = 'àâæçéèêêîïôœùûüÿ'\n",
    "    all_lang_chars = base_en + italian + french \n",
    "    small_chars = list(set(list(all_lang_chars)))\n",
    "    small_chars.sort() \n",
    "    big_chars = list(set(list(all_lang_chars.upper())))\n",
    "    big_chars.sort()\n",
    "    small_chars += special_chars\n",
    "    letters_string = ''\n",
    "    letters = small_chars + big_chars\n",
    "    for letter in letters:\n",
    "        letters_string += letter\n",
    "    return small_chars,big_chars,letters_string\n",
    "\n",
    "alphabet, _, _ = define_alphabet()\n",
    "print(alphabet, len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fits labelBinarizer obj to alphabet so it can make one-hot encoding\n",
    "# c = unique characters between eng, fre, ital = 93\n",
    "le = LabelBinarizer()\n",
    "le.fit(alphabet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "# d = 100\n",
    "# eta = 0.1\n",
    "# input_dim = 455\n",
    "\n",
    "# W1 = np.random.uniform(size=(d, input_dim))\n",
    "# bias1 = np.random.uniform(size=[d,1])\n",
    "# W2 = np.random.uniform(size=(3,d))\n",
    "# bias2 = np.random.uniform(size=[3, 1])\n",
    "\n",
    "def forward(x, W1, bias1, W2, bias2):\n",
    "    hidden_layer = sigmoid_array(np.dot(W1, x.T) + bias1)\n",
    "    y = softmax(np.dot(W2, hidden_layer) + bias2)\n",
    "    return hidden_layer, y\n",
    "\n",
    "# h_layer, y_pred = forward(x, W1, bias1, W2, bias2)\n",
    "# y_test = np.array([1,0, 0]).reshape(3,1)\n",
    "# print(W1.shape, bias1.shape, W2.shape, bias2.shape,\n",
    "#       h_layer.shape, y_pred.shape, y_test.shape)\n",
    "\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions so far\n",
    "- how to know which index in softmax corresponds to which language?\n",
    "- \"Shuffle   your   training   data   to   prevent   the   network   overfitting   to   one   output   class.\"??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_chars(s, num_chars=5):\n",
    "    n = len(s)\n",
    "    return [s[i:(i+num_chars)] for i in range(n-4)]\n",
    "\n",
    "def binarize(seq_str):\n",
    "    nseq = len(seq_str)\n",
    "    return np.array([le.transform(list(seq_str[i])) for i in range(nseq)]).reshape(nseq, input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for backpropogation\n",
    "def grad_l_wrt_y(y_pred, y_true):\n",
    "    return y_pred - y_true #(3,1)\n",
    "\n",
    "def grad_l_wrt_b2(y_pred, y_true):\n",
    "    vec = []\n",
    "    for j, val in enumerate(list(y_pred)):\n",
    "        counter = 0\n",
    "        for i, val2 in enumerate(list(y_pred)):\n",
    "            if i == j:\n",
    "                counter += (y_pred[i] - y_test[i])*(y_pred[i])*(1-y_pred[j])\n",
    "            else:\n",
    "                counter += (y_pred[i] - y_test[i])*(y_pred[i])*(-y_pred[j])\n",
    "        vec.append(counter)\n",
    "\n",
    "                     \n",
    "    return np.array(vec)\n",
    "\n",
    "def grad_l_wrt_w2(grad_b2, hidden_layer):\n",
    "    return grad_b2.dot(hidden_layer.T) #(3,1)*(1,d) is (3,d)\n",
    "\n",
    "def grad_l_wrt_h(grad_b2, W2):\n",
    "    return np.dot(W2.T, grad_b2) #(d,3)*(3,1) is (d,1)\n",
    "\n",
    "def grad_l_wrt_h_tilde(grad_h, h_layer):\n",
    "    return grad_h * ((h_layer) * (1-h_layer)) # (d,1)(elt *)(d,1) is (d,1)\n",
    "#     vec = []\n",
    "#     for i in range(len(h_layer)):\n",
    "# #         print(\"mult:\", grad_h[i], (h_layer[i] * (1-h_layer[i])))\n",
    "#         vec.append(grad_h[i] * (h_layer[i] * (1-h_layer[i])))\n",
    "#     return np.array(vec)\n",
    "\n",
    "def grad_l_wrt_w1(grad_h_tilde, input_x):\n",
    "#     x = input_x.reshape([455,])\n",
    "#     h = grad_h_tilde.reshape([100,])\n",
    "    \n",
    "#     _, a = np.nonzero(input_x)\n",
    "#     b = np.where(grad_h_tilde == 0)\n",
    "#     print(a, b)\n",
    "#     print('nonzero vals', x[a])\n",
    "#     print('nonzero?', np.dot(grad_h_tilde, input_x)[np.nonzero(np.dot(grad_h_tilde, input_x))])\n",
    "#     return np.outer(grad_h_tilde.reshape([100,]), input_x.reshape([455,]))\n",
    "    return np.dot(grad_h_tilde, input_x) #(d,1)*(1,5c) is (d,5c)\n",
    "\n",
    "def grad_l_wrt_b1(grad_h_tilde):\n",
    "    return grad_h_tilde\n",
    "\n",
    "def backprop(y_pred, y_test, h_layer, input_x, W1, W2, bias1, bias2, eta=.1):   \n",
    "\n",
    "    grad_l_y = grad_l_wrt_y(y_pred, y_test)\n",
    "#     print('grad_l_y shape is {}'.format(grad_l_y.shape))\n",
    "#     print(grad_l_y)\n",
    "    grad_l_b2 = grad_l_wrt_b2(y_pred, y_test)\n",
    "#     print(grad_l_b2)\n",
    "#     print('grad_l_b2 shape is {}'.format(grad_l_b2.shape))\n",
    "    grad_l_w2 = grad_l_wrt_w2(grad_l_b2, h_layer)\n",
    "#     print(grad_l_w2)\n",
    "#     print('grad_l_w2 shape is {}'.format(grad_l_w2.shape))\n",
    "    grad_l_h = grad_l_wrt_h(grad_l_b2, W2)\n",
    "#     print(grad_l_h)\n",
    "#     print('grad_l_h shape is {}'.format(grad_l_h.shape))\n",
    "    grad_h_tilde = grad_l_wrt_h_tilde(grad_l_h, h_layer)\n",
    "#     print(grad_h_tilde)\n",
    "#     print('grad_h_tilde shape is {}'.format(grad_h_tilde.shape))\n",
    "    grad_l_w1 = grad_l_wrt_w1(grad_h_tilde, input_x)\n",
    "#     print(grad_l_w1)\n",
    "#     print('grad_l_w1 shape is {}'.format(grad_l_w1.shape))\n",
    "    grad_l_b1 = grad_l_wrt_b1(grad_h_tilde)\n",
    "#     print(grad_l_b1)\n",
    "#     print('grad_l_b1 shape is {}'.format(grad_l_b1.shape))\n",
    "\n",
    "    # no error\n",
    "#     print(np.nonzero(W1))\n",
    "#     print(W1[np.nonzero(W1)], W1.shape)\n",
    "    # confirms that there are many nonzero, but mostly 0, and that eta * grad works\n",
    "#     print(grad_l_w1[np.nonzero(grad_l_w1)], (eta * grad_l_w1)[np.nonzero(eta*grad_l_w1)])\n",
    "    W1 = W1 -  eta * grad_l_w1\n",
    "    # this shows all rows exaclty the same except for 5, which corresponds to the 5 \"1\" values \n",
    "    # that were in the input\n",
    "#     print(sum(sum(W1t==W1)))\n",
    "    W2 = W2 - eta * grad_l_w2\n",
    "    bias2 = bias2 - eta * grad_l_b2\n",
    "    bias1 = bias1 - eta * grad_l_b1\n",
    "#     print(np.nonzero(grad_l_w1))\n",
    "#     print(grad_l_w1[np.nonzero(grad_l_w1)], grad_l_w1.shape)\n",
    "#     print(np.nonzero(W1))\n",
    "#     print(W1[np.nonzero(W1)], W1.shape)\n",
    "    return W1, bias1, W2, bias2\n",
    "\n",
    "\n",
    "# print('before backprop', W1, bias1, W2, bias2)\n",
    "# print('post backprop', backprop(y_pred, y_test, h_layer, x, W1, W2,\n",
    "#                                bias1, bias2))\n",
    "\n",
    "# w1t, b1t, w2t, b2t = backprop(y_pred, y_test, h_layer, x_test, W1, W2, bias1, bias2, eta)\n",
    "\n",
    "\n",
    "# # # these are exactly equal sadly...\n",
    "# sum(sum(w1t!=W1))\n",
    "# # np.nonzero(w1t)\n",
    "\n",
    "\n",
    "\n",
    "# np.random.seed(1)\n",
    "filename = \"languageIdentification.data/train\"\n",
    "d = 100\n",
    "eta = 0.1\n",
    "input_dim = 455\n",
    "\n",
    "W1 = np.random.uniform(size=(d, input_dim))\n",
    "bias1 = np.random.uniform(size=[d,1])\n",
    "W2 = np.random.uniform(size=(3,d))\n",
    "bias2 = np.random.uniform(size=[3, 1])\n",
    "eta = 0.1\n",
    "li = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jarvm/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4325922117979694, 0.32155249967870453, 0.32155249967870453]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    with open(filename, 'r') as handle:\n",
    "        # init weights and biases\n",
    "\n",
    "        num_chances = 0\n",
    "        num_correct = 0\n",
    "        for line in handle:\n",
    "#             print(bias2, line)\n",
    "            num_chances += 1\n",
    "            s = line.split()\n",
    "            label = s[0]\n",
    "            sentence = ' '.join(s[1:]).lower()\n",
    "            encode_mat = binarize(seq_chars(sentence))\n",
    "    #         print(np.nonzero(encode_mat)[1])\n",
    "    #         print(\"next line\")\n",
    "            # accumulate pred for each line, take avg, then get error\n",
    "            pred = np.zeros(3)\n",
    "\n",
    "            if label == \"ENGLISH\":\n",
    "                y_test = np.array([0,1,0])\n",
    "            elif label == \"ITALIAN\":\n",
    "                y_test = np.array([1,0,0])\n",
    "            else:\n",
    "                y_test = np.array([0,0,1])\n",
    "\n",
    "    #         # for each 5 character encoder vector\n",
    "            num_rows = 0\n",
    "            for row in range(len(encode_mat)):\n",
    "                input_x = encode_mat[row,:].reshape(1,455)\n",
    "                num_rows += 1\n",
    "    #             # forward prop\n",
    "                h_layer, y_pred = forward(input_x, W1, bias1, W2, bias2)\n",
    "    #             # accumulate softmax\n",
    "                pred += y_pred.reshape(3,)\n",
    "#                 print(pred)\n",
    "\n",
    "   \n",
    "\n",
    "    # #         # DO THIS 3 TIMES for testing. then use W1, bias1, \n",
    "    # #         # W2, bias2 on testing data\n",
    "    # #         # THIS WOULD BE DONE WHEN TESTING\n",
    "    # #         # after a sentence is done\n",
    "            pred_avg = pred / num_rows  # avg softmax over each 5 letter pred of sentence\n",
    "    #         loss = np.square(pred_avg-y_test).sum()\n",
    "    #         if loss > .3:\n",
    "    #             print(loss, pred_avg, y_test, line)\n",
    "    #         print('sentence done', pred, num_rows, pred_avg)\n",
    "            pred_final = np.zeros(3)\n",
    "    # #         # index of max prob is chosen to be one\n",
    "            pred_final[np.argmax(pred_avg)] = 1\n",
    "    #         print(pred_final, y_test)\n",
    "    # #         # if correct guess\n",
    "            if np.all(pred_final - y_test == np.array([0,0,0])):\n",
    "                num_correct += 1\n",
    "#                 print('yay', label)\n",
    "            # im guessing we should backprop after each sentence, not after each 5 character thingy\n",
    "            W1, bias1, W2, bias2 = backprop(pred_avg.reshape(3,1), y_test, h_layer, \n",
    "                                                input_x, W1, W2, bias1, bias2, eta=eta) \n",
    "\n",
    "    # #     # after going through all lines\n",
    "    accuracy = num_correct / num_chances\n",
    "\n",
    "    li.append(accuracy)\n",
    "print(li) # hmm this should be getting better\n",
    "# the problem is that I predict [1,0,0] all the time if i dont train....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate on dev\n",
    "# filename_test = \"languageIdentification.data/test\"\n",
    "# final_ans = []\n",
    "# with open(filename_test, 'r') as handle:\n",
    "#     # init weights and biases\n",
    "\n",
    "#     num_chances = 0\n",
    "#     num_correct = 0\n",
    "#     for line in handle:\n",
    "# #             print(bias2, line)\n",
    "#         num_chances += 1\n",
    "#         s = line.split()\n",
    "# #         label = s[0]\n",
    "#         sentence = ' '.join(s).lower()\n",
    "#         encode_mat = binarize(seq_chars(sentence))\n",
    "# #         print(np.nonzero(encode_mat)[1])\n",
    "# #         print(\"next line\")\n",
    "#         # accumulate pred for each line, take avg, then get error\n",
    "#         pred = np.zeros(3)\n",
    "\n",
    "# #         if label == \"ENGLISH\":\n",
    "# #             y_test = np.array([0,1,0])\n",
    "# #         elif label == \"ITALIAN\":\n",
    "# #             y_test = np.array([1,0,0])\n",
    "# #         else:\n",
    "# #             y_test = np.array([0,0,1])\n",
    "\n",
    "# #         # for each 5 character encoder vector\n",
    "#         num_rows = 0\n",
    "#         for row in range(len(encode_mat)):\n",
    "#             input_x = encode_mat[row,:].reshape(1,455)\n",
    "#             num_rows += 1\n",
    "# #             # forward prop\n",
    "#             h_layer, y_pred = forward(input_x, W1, bias1, W2, bias2)\n",
    "# #             # accumulate softmax\n",
    "#             pred += y_pred.reshape(3,)\n",
    "# #             print(pred)\n",
    "\n",
    "\n",
    "\n",
    "# # #         # DO THIS 3 TIMES for testing. then use W1, bias1, \n",
    "# # #         # W2, bias2 on testing data\n",
    "# # #         # THIS WOULD BE DONE WHEN TESTING\n",
    "# # #         # after a sentence is done\n",
    "#         pred_avg = pred / num_rows  # avg softmax over each 5 letter pred of sentence\n",
    "# #         loss = np.square(pred_avg-y_test).sum()\n",
    "# #         if loss > .3:\n",
    "# #             print(loss, pred_avg, y_test, line)\n",
    "# #         print('sentence done', pred, num_rows, pred_avg)\n",
    "#         pred_final = np.zeros(3)\n",
    "# # #         # index of max prob is chosen to be one\n",
    "#         pred_final[np.argmax(pred_avg)] = 1\n",
    "#         final_ans.append(pred_final)\n",
    "# #         if pred_final == np.array([0,1,0]):\n",
    "# #             lab = 'ENGLISH'\n",
    "# #         elif label == np.array([1,0,0]):\n",
    "# #             lab = 'ITALIAN'\n",
    "# #         else:\n",
    "# #             lab = 'FRENCH'\n",
    "\n",
    "# #         print(pred_final, y_test)\n",
    "# # #         # if correct guess\n",
    "# #         if np.all(pred_final - y_test == np.array([0,0,0])):\n",
    "# #             num_correct += 1\n",
    "# #                 print('yay', label)\n",
    "#         # im guessing we should backprop after each sentence, not after each 5 character thingy\n",
    "# #         W1, bias1, W2, bias2 = backprop(pred_avg.reshape(3,1), y_test, h_layer, \n",
    "# #                                             input_x, W1, W2, bias1, bias2, eta=eta) \n",
    "\n",
    "# # #     # after going through all lines\n",
    "# # accuracy = num_correct / num_chances\n",
    "\n",
    "# # li.append(accuracy)\n",
    "# # print(li) # hmm this should be getting better\n",
    "# # the problem is that I predict [1,0,0] all the time if i dont train....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # example of an input 'x' which is 5c,1\n",
    "# # c = unique characters between eng, fre, ital = 93\n",
    "# le = LabelEncoder()\n",
    "# oe = OneHotEncoder(sparse=False)\n",
    "# int_encoded = le.fit_transform(np.array(alphabet))\n",
    "# int_encoded = int_encoded.reshape(len(int_encoded),1)\n",
    "# # s = list('ggéré'.lower())\n",
    "# # x = le.transform(s)\n",
    "# x = oe.fit_transform(int_encoded)\n",
    "# # print(x, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfRGRAPl/UsuE8vGuMIkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114a220b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(li))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
